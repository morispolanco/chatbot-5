import streamlit as st
import requests
import json

# Mostrar tÃ­tulo y descripciÃ³n
st.title("ğŸ“ Asistente de BÃºsqueda de Becas")
st.write(
    "Este asistente te ayudarÃ¡ a encontrar becas de estudio basadas en tus intereses y antecedentes. "
    "Utilizamos inteligencia artificial para procesar tu informaciÃ³n y realizar bÃºsquedas personalizadas."
)

# Obtener claves API de los secretos de Streamlit
together_api_key = st.secrets["TOGETHER_API_KEY"]
serper_api_key = st.secrets["SERPER_API_KEY"]

# Configurar los endpoints de API y headers
together_url = "https://api.together.xyz/v1/chat/completions"
serper_url = "https://google.serper.dev/search"

together_headers = {
    "Authorization": f"Bearer {together_api_key}",
    "Content-Type": "application/json"
}

serper_headers = {
    "X-API-KEY": serper_api_key,
    "Content-Type": "application/json"
}

# FunciÃ³n para realizar bÃºsqueda en Google usando la API de Serper
def busqueda_google(consulta):
    payload = json.dumps({"q": consulta})
    response = requests.post(serper_url, headers=serper_headers, data=payload)
    return response.json()

# FunciÃ³n para obtener respuesta del LLM usando la API de Together
def obtener_respuesta_llm(mensajes):
    payload = {
        "model": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        "messages": mensajes,
        "max_tokens": 1024,
        "temperature": 0.7,
        "top_p": 0.7,
        "top_k": 50,
        "repetition_penalty": 1,
        "stop": ["<|eot_id|>", "<|eom_id|>"],
        "stream": True
    }
    return requests.post(together_url, headers=together_headers, json=payload, stream=True)

# Inicializar variables de estado de sesiÃ³n
if "mensajes" not in st.session_state:
    st.session_state.mensajes = []
if "info_usuario" not in st.session_state:
    st.session_state.info_usuario = {}
if "etapa_dialogo" not in st.session_state:
    st.session_state.etapa_dialogo = 0

# Lista de preguntas
preguntas = [
    "Â¿En quÃ© campo de estudio estÃ¡s interesado?",
    "Â¿En quÃ© paÃ­s o regiÃ³n te gustarÃ­a estudiar?",
    "Â¿QuÃ© nivel acadÃ©mico estÃ¡s buscando? (Por ejemplo: Licenciatura, MaestrÃ­a, Doctorado, Postdoctorado)",
    "Â¿CuÃ¡l es tu nacionalidad?",
    "Â¿EstÃ¡s interesado solo en becas especÃ­ficas para tu nacionalidad? (Responde SÃ­ o No)"
]

# Mostrar mensajes del chat
for mensaje in st.session_state.mensajes:
    with st.chat_message(mensaje["role"]):
        st.markdown(mensaje["content"])

# DiÃ¡logo principal
if st.session_state.etapa_dialogo < len(preguntas):
    # Mostrar la pregunta actual
    with st.chat_message("assistant"):
        st.markdown(preguntas[st.session_state.etapa_dialogo])
    
    # Esperar la respuesta del usuario
    respuesta_usuario = st.chat_input("Tu respuesta aquÃ­")
    
    if respuesta_usuario:
        # Mostrar la respuesta del usuario
        with st.chat_message("user"):
            st.markdown(respuesta_usuario)
        
        # Guardar la respuesta
        st.session_state.mensajes.append({"role": "user", "content": respuesta_usuario})
        if st.session_state.etapa_dialogo == 0:
            st.session_state.info_usuario["campo"] = respuesta_usuario
        elif st.session_state.etapa_dialogo == 1:
            st.session_state.info_usuario["ubicacion"] = respuesta_usuario
        elif st.session_state.etapa_dialogo == 2:
            st.session_state.info_usuario["nivel"] = respuesta_usuario
        elif st.session_state.etapa_dialogo == 3:
            st.session_state.info_usuario["nacionalidad"] = respuesta_usuario
        elif st.session_state.etapa_dialogo == 4:
            st.session_state.info_usuario["especifica_nacionalidad"] = respuesta_usuario.lower() in ["sÃ­", "si", "yes", "y", "s"]
        
        # Avanzar a la siguiente etapa
        st.session_state.etapa_dialogo += 1
        st.rerun()

elif st.session_state.etapa_dialogo == len(preguntas):
    # Procesar la informaciÃ³n y buscar becas
    info_usuario = st.session_state.info_usuario
    consulta_busqueda = f"becas para {info_usuario['nivel']} en {info_usuario['campo']} en {info_usuario['ubicacion']}"
    if info_usuario.get('especifica_nacionalidad', False):
        consulta_busqueda += f" para estudiantes de {info_usuario['nacionalidad']}"

    try:
        resultados_busqueda = busqueda_google(consulta_busqueda)
    except Exception as e:
        st.error(f"Error durante la bÃºsqueda en Google: {str(e)}")
        resultados_busqueda = {"organic": []}

    contexto = "Resultados de bÃºsqueda para becas:\n"
    for i, resultado in enumerate(resultados_busqueda.get('organic', [])[:5], 1):
        contexto += f"{i}. {resultado.get('title', 'Sin tÃ­tulo')}: {resultado.get('snippet', 'Sin descripciÃ³n')} [Enlace: {resultado.get('link', 'Sin enlace')}]\n"

    prompt = f"""
    BasÃ¡ndote en las siguientes preferencias del usuario y los resultados de bÃºsqueda, recomienda becas adecuadas:
    
    Preferencias del usuario:
    - Campo de estudio: {info_usuario['campo']}
    - UbicaciÃ³n de estudio deseada: {info_usuario['ubicacion']}
    - Nivel acadÃ©mico: {info_usuario['nivel']}
    - Nacionalidad: {info_usuario['nacionalidad']}
    - Solo interesado en becas especÃ­ficas para su nacionalidad: {"SÃ­" if info_usuario.get('especifica_nacionalidad', False) else "No"}

    {contexto}

    Por favor, proporciona una respuesta detallada en espaÃ±ol con:
    1. Las oportunidades de becas mÃ¡s relevantes.
    2. Enlaces directos a las instituciones que ofrecen estas becas.
    3. Breves explicaciones de por quÃ© recomiendas cada instituciÃ³n o beca.
    4. Cualquier consejo adicional para el usuario basado en sus preferencias.
    """

    mensajes = [
        {"role": "system", "content": "Eres un asistente de bÃºsqueda de becas muy Ãºtil. Proporciona informaciÃ³n detallada y precisa sobre becas basada en las preferencias del usuario y los resultados de la bÃºsqueda. Responde siempre en espaÃ±ol."},
        {"role": "user", "content": prompt}
    ]

    with st.chat_message("assistant"):
        marcador_mensaje = st.empty()
        respuesta_completa = ""

        try:
            respuesta = obtener_respuesta_llm(mensajes)

            for linea in respuesta.iter_lines():
                if linea:
                    try:
                        datos = linea.decode('utf-8').split('data: ', 1)
                        if len(datos) > 1:
                            fragmento = json.loads(datos[1])
                            if fragmento['choices'][0]['finish_reason'] is None:
                                contenido = fragmento['choices'][0]['delta'].get('content', '')
                                respuesta_completa += contenido
                                marcador_mensaje.markdown(respuesta_completa + "â–Œ")
                    except json.JSONDecodeError:
                        continue

            if not respuesta_completa:
                respuesta_completa = "Lo siento, no pude generar una respuesta. Por favor, intenta de nuevo."

        except Exception as e:
            respuesta_completa = f"OcurriÃ³ un error al procesar tu solicitud: {str(e)}"

        marcador_mensaje.markdown(respuesta_completa)

    st.session_state.mensajes.append({"role": "assistant", "content": respuesta_completa})
    st.session_state.etapa_dialogo += 1

# BotÃ³n para iniciar una nueva bÃºsqueda de becas
if st.button("Iniciar nueva bÃºsqueda de becas"):
    st.session_state.mensajes = []
    st.session_state.info_usuario = {}
    st.session_state.etapa_dialogo = 0
    st.rerun()
